#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì£¼ì‹ ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ (ê³ ì„±ëŠ¥ ë²„ì „)
out_krx_parallel í´ë”ì˜ CSV íŒŒì¼ë“¤ì„ ë©€í‹°ìŠ¤ë ˆë”© ë° ë²Œí¬ ì¸ì„œíŠ¸ë¡œ ë¹ ë¥´ê²Œ ì €ì¥
"""

import os
import pandas as pd
import mysql.connector
from mysql.connector import Error
import logging
import glob
from tqdm import tqdm
import time
from datetime import datetime
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('stock_data_processing.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class StockDataProcessor:
    def __init__(self, db_config):
        self.db_config = db_config
        self.connection = None
        self.cursor = None
        self.csv_dir = "./out_krx_parallel"
        self.batch_size = 1000  # ë²Œí¬ ì¸ì„œíŠ¸ ë°°ì¹˜ í¬ê¸°
        self.lock = threading.Lock()  # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½
        
        # ì¢…ëª© ì •ë³´ (ì£¼ìš” ì¢…ëª©ë“¤)
        self.stock_info = {
            '005930': 'ì‚¼ì„±ì „ì', '000660': 'SKí•˜ì´ë‹‰ìŠ¤', '035420': 'NAVER',
            '035720': 'ì¹´ì¹´ì˜¤', '051910': 'LGí™”í•™', '006400': 'ì‚¼ì„±SDI',
            '033780': 'KT&G', '003550': 'LG', '012330': 'í˜„ëŒ€ëª¨ë¹„ìŠ¤',
            '017670': 'SKí…”ë ˆì½¤', '090430': 'ì•„ëª¨ë ˆí¼ì‹œí”½', '009830': 'í•œí™”ì†”ë£¨ì…˜',
            '012450': 'í•œí™”ì—ì–´ë¡œìŠ¤í˜ì´ìŠ¤', '010140': 'ì‚¼ì„±ì¤‘ê³µì—…', '088350': 'í•œí™”ì‹œìŠ¤í…œ',
            '034020': 'ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°', '003490': 'ëŒ€í•œí•­ê³µ', '028260': 'ì‚¼ì„±ë¬¼ì‚°',
            '066570': 'LGì „ì', '009150': 'ì‚¼ì„±ìƒëª…', '096770': 'SKì´ë…¸ë² ì´ì…˜',
            '024110': 'ê¸°ì—…ì€í–‰', '316140': 'ìš°ë¦¬ê¸ˆìœµì§€ì£¼', '011200': 'HMM',
            '010130': 'ê³ ë ¤ì•„ì—°', '055550': 'ì‹ í•œì§€ì£¼', '030200': 'KT',
            '011170': 'ì˜¤ëšœê¸°', '004990': 'ë¡¯ë°ì§€ì£¼', '336260': 'ë‘ì‚°ë¡œë³´í‹±ìŠ¤',
            '029780': 'ì‚¼ì„±ì¹´ë“œ', '000120': 'CJëŒ€í•œí†µìš´', '000720': 'í˜„ëŒ€ê±´ì„¤',
            '086280': 'í˜„ëŒ€ê¸€ë¡œë¹„ìŠ¤', '002380': 'í•œêµ­ê³µí•­ê³µì‚¬', '004020': 'í˜„ëŒ€ì œì² ',
            '011790': 'SKC', '006360': 'GSê±´ì„¤', '036570': 'ì—”ì”¨ì†Œí”„íŠ¸',
            '035250': 'ê°•ì›ëœë“œ', '402340': 'SKìŠ¤í€˜ì–´', '018260': 'ì‚¼ì„±ì—ìŠ¤ë””ì—ìŠ¤',
            '023530': 'ë¡¯ë°ì‡¼í•‘', '008770': 'í˜¸í…”ì‹ ë¼', '069960': 'í˜„ëŒ€ë°±í™”ì ',
            '035760': 'CJ ENM', '139480': 'ì´ë§ˆíŠ¸', '010950': 'S-Oil',
            '456040': 'í•œí™”ì—ì–´ë¡œìŠ¤í˜ì´ìŠ¤', '375500': 'DLì´ì•¤ì”¨', '064350': 'í˜„ëŒ€ë¡œí…œ',
            '018880': 'í•œì˜¨ì‹œìŠ¤í…œ', '007070': 'GSë¦¬í…Œì¼', '120110': 'ì½”ì˜¤ë¡±ì¸ë”',
            '011210': 'í˜„ëŒ€ìœ„ì•„', '000880': 'í•œí™”', '006650': 'ëŒ€í•œë‰´íŒœ',
            '001450': 'í˜„ëŒ€í•´ìƒ', '180640': 'í•œì§„ì¹¼', '004000': 'ë¡¯ë°ì •ë°€í™”í•™',
            '282330': 'BGFë¦¬í…Œì¼', '112610': 'ì”¨ì—ìŠ¤ìœˆë“œ', '009240': 'í•œìƒ˜',
            '047040': 'ëŒ€ìš°ê±´ì„¤', '161390': 'í•œêµ­íƒ€ì´ì–´ì•¤í…Œí¬ë†€ë¡œì§€', '089860': 'ë¡¯ë°ë Œíƒˆ',
            '008930': 'í•œë¯¸ì‚¬ì´ì–¸ìŠ¤', '251270': 'ë„·ë§ˆë¸”', '032830': 'ì‚¼ì„±ìƒëª…',
            '028670': 'íŒ¬ì˜¤ì…˜', '051900': 'LGìƒí™œê±´ê°•', '016360': 'ì‚¼ì„±ì¦ê¶Œ',
            '071050': 'í•œêµ­ê¸ˆìœµì§€ì£¼', '047810': 'í•œêµ­í•­ê³µìš°ì£¼', '035720': 'ì¹´ì¹´ì˜¤',
            '086790': 'í•˜ë‚˜ê¸ˆìœµì§€ì£¼', '005940': 'NHíˆ¬ìì¦ê¶Œ', '078930': 'GS',
            '097950': 'CJì œì¼ì œë‹¹', '036460': 'í•œêµ­ê°€ìŠ¤ê³µì‚¬', '005830': 'DBì†í•´ë³´í—˜',
            '034730': 'SK', '128940': 'í•œë¯¸ì•½í’ˆ', '035420': 'NAVER',
            '011780': 'ê¸ˆí˜¸ì„ìœ ', '069500': 'KODEX 200', '005380': 'í˜„ëŒ€ì°¨',
            '005490': 'í¬ìŠ¤ì½”í™€ë”©ìŠ¤', '015760': 'í•œêµ­ì „ë ¥', '204320': 'ë§Œë„',
            '000150': 'ë‘ì‚°', '010120': 'LS ELECTRIC', '010620': 'í˜„ëŒ€ë¯¸í¬ì¡°ì„ ',
            '009540': 'í•œêµ­ì¡°ì„ í•´ì–‘', '000810': 'ì‚¼ì„±í™”ì¬', '326030': 'SKë°”ì´ì˜¤íŒœ'
        }

    def connect_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        try:
            self.connection = mysql.connector.connect(**self.db_config)
            self.cursor = self.connection.cursor()
            logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
            return True
        except Error as e:
            logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    def disconnect_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•´ì œ"""
        if self.cursor:
            self.cursor.close()
        if self.connection:
            self.connection.close()
        logger.info("ğŸ”Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•´ì œ")

    def get_csv_files(self):
        """CSV íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        pattern = os.path.join(self.csv_dir, "*.csv")
        csv_files = glob.glob(pattern)
        logger.info(f"ğŸ“ CSV íŒŒì¼ {len(csv_files)}ê°œ ë°œê²¬")
        return csv_files

    def parse_stock_code_from_filename(self, filename):
        """íŒŒì¼ëª…ì—ì„œ ì¢…ëª©ì½”ë“œì™€ ë°ì´í„° íƒ€ì… ì¶”ì¶œ"""
        basename = os.path.basename(filename)
        match = re.match(r'(\d{6})_([DWM])\.csv', basename)
        if match:
            return match.group(1), match.group(2)
        return None, None

    def load_csv_data(self, filepath):
        """CSV íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬ (ë²Œí¬ ì²˜ë¦¬ìš©)"""
        try:
            df = pd.read_csv(filepath, encoding='utf-8')
            
            # ì»¬ëŸ¼ëª… ì •ê·œí™”
            column_mapping = {
                'date': 'date',
                'open': 'open',
                'high': 'high', 
                'low': 'low',
                'close': 'close',
                'volume': 'volume'
            }
            
            df = df.rename(columns=column_mapping)
            df['date'] = pd.to_datetime(df['date'])
            
            # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
            numeric_columns = ['open', 'high', 'low', 'close', 'volume']
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            df = df.dropna()
            df = df.sort_values('date')
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {filepath}: {e}")
            return None

    def bulk_insert_daily_data(self, stock_code, df):
        """ì¼ë³„ ë°ì´í„° ë²Œí¬ ì‚½ì…"""
        if df.empty:
            return 0
            
        try:
            # ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
            total_rows = len(df)
            inserted_count = 0
            
            for i in range(0, total_rows, self.batch_size):
                batch_df = df.iloc[i:i+self.batch_size]
                
                # ë²Œí¬ ì¸ì„œíŠ¸ ì¿¼ë¦¬ ìƒì„±
                placeholders = ', '.join(['(%s, %s, %s, %s, %s, %s, %s, NOW(), NOW())'] * len(batch_df))
                query = f"""
                INSERT INTO stock_daily_prices 
                (stock_symbol, trade_date, open_price, high_price, low_price, close_price, volume, created_at, updated_at)
                VALUES {placeholders}
                ON DUPLICATE KEY UPDATE
                open_price = VALUES(open_price),
                high_price = VALUES(high_price),
                low_price = VALUES(low_price),
                close_price = VALUES(close_price),
                volume = VALUES(volume),
                updated_at = NOW()
                """
                
                # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                values = []
                for _, row in batch_df.iterrows():
                    values.extend([
                        stock_code,
                        row['date'].date(),
                        float(row['open']),
                        float(row['high']),
                        float(row['low']),
                        float(row['close']),
                        int(row['volume'])
                    ])
                
                with self.lock:  # ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë³´ì¥
                    self.cursor.execute(query, values)
                    inserted_count += len(batch_df)
            
            return inserted_count
            
        except Exception as e:
            logger.error(f"âŒ ì¼ë³„ ë°ì´í„° ë²Œí¬ ì‚½ì… ì‹¤íŒ¨ {stock_code}: {e}")
            return 0

    def bulk_insert_weekly_data(self, stock_code, df):
        """ì£¼ë³„ ë°ì´í„° ë²Œí¬ ì‚½ì…"""
        if df.empty:
            return 0
            
        try:
            # ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
            total_rows = len(df)
            inserted_count = 0
            
            for i in range(0, total_rows, self.batch_size):
                batch_df = df.iloc[i:i+self.batch_size]
                
                # ë²Œí¬ ì¸ì„œíŠ¸ ì¿¼ë¦¬ ìƒì„±
                placeholders = ', '.join(['(%s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())'] * len(batch_df))
                query = f"""
                INSERT INTO stock_weekly_prices 
                (stock_symbol, week_start_date, week_end_date, open_price, high_price, low_price, close_price, volume, created_at, updated_at)
                VALUES {placeholders}
                ON DUPLICATE KEY UPDATE
                open_price = VALUES(open_price),
                high_price = VALUES(high_price),
                low_price = VALUES(low_price),
                close_price = VALUES(close_price),
                volume = VALUES(volume),
                updated_at = NOW()
                """
                
                # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                values = []
                for _, row in batch_df.iterrows():
                    week_start = row['date'].date()
                    week_end = week_start + pd.Timedelta(days=4)
                    
                    values.extend([
                        stock_code,
                        week_start,
                        week_end,
                        float(row['open']),
                        float(row['high']),
                        float(row['low']),
                        float(row['close']),
                        int(row['volume'])
                    ])
                
                with self.lock:  # ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë³´ì¥
                    self.cursor.execute(query, values)
                    inserted_count += len(batch_df)
            
            return inserted_count
            
        except Exception as e:
            logger.error(f"âŒ ì£¼ë³„ ë°ì´í„° ë²Œí¬ ì‚½ì… ì‹¤íŒ¨ {stock_code}: {e}")
            return 0

    def bulk_insert_monthly_data(self, stock_code, df):
        """ì›”ë³„ ë°ì´í„° ë²Œí¬ ì‚½ì…"""
        if df.empty:
            return 0
            
        try:
            # ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
            total_rows = len(df)
            inserted_count = 0
            
            for i in range(0, total_rows, self.batch_size):
                batch_df = df.iloc[i:i+self.batch_size]
                
                # ë²Œí¬ ì¸ì„œíŠ¸ ì¿¼ë¦¬ ìƒì„±
                placeholders = ', '.join(['(%s, %s, %s, %s, %s, %s, %s, NOW(), NOW())'] * len(batch_df))
                query = f"""
                INSERT INTO stock_monthly_prices 
                (stock_symbol, year_month_period, open_price, high_price, low_price, close_price, volume, created_at, updated_at)
                VALUES {placeholders}
                ON DUPLICATE KEY UPDATE
                open_price = VALUES(open_price),
                high_price = VALUES(high_price),
                low_price = VALUES(low_price),
                close_price = VALUES(close_price),
                volume = VALUES(volume),
                updated_at = NOW()
                """
                
                # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                values = []
                for _, row in batch_df.iterrows():
                    year_month = row['date'].strftime('%Y-%m')
                    
                    values.extend([
                        stock_code,
                        year_month,
                        float(row['open']),
                        float(row['high']),
                        float(row['low']),
                        float(row['close']),
                        int(row['volume'])
                    ])
                
                with self.lock:  # ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë³´ì¥
                    self.cursor.execute(query, values)
                    inserted_count += len(batch_df)
            
            return inserted_count
            
        except Exception as e:
            logger.error(f"âŒ ì›”ë³„ ë°ì´í„° ë²Œí¬ ì‚½ì… ì‹¤íŒ¨ {stock_code}: {e}")
            return 0

    def update_stock_master(self, stock_code, has_daily=False, has_weekly=False, has_monthly=False):
        """ì£¼ì‹ ë§ˆìŠ¤í„° ì •ë³´ ì—…ë°ì´íŠ¸"""
        try:
            check_query = "SELECT id FROM stock_master WHERE symbol = %s"
            with self.lock:  # ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë³´ì¥
                self.cursor.execute(check_query, (stock_code,))
                exists = self.cursor.fetchone()
            
            if not exists:
                insert_query = """
                INSERT INTO stock_master (symbol, name, market, sector, has_daily_data, has_weekly_data, has_monthly_data, created_at, updated_at)
                VALUES (%s, %s, %s, %s, %s, %s, %s, NOW(), NOW())
                """
                
                values = (
                    stock_code,
                    self.stock_info.get(stock_code, f'ì¢…ëª©{stock_code}'),
                    'KOSPI',
                    'ê¸°íƒ€',
                    has_daily,
                    has_weekly,
                    has_monthly
                )
                
                with self.lock:  # ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë³´ì¥
                    self.cursor.execute(insert_query, values)
                    logger.info(f"âœ… ìƒˆ ì¢…ëª© ì¶”ê°€: {stock_code}")
            else:
                # ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸
                update_query = """
                UPDATE stock_master 
                SET has_daily_data = %s, has_weekly_data = %s, has_monthly_data = %s, updated_at = NOW()
                WHERE symbol = %s
                """
                
                values = (has_daily, has_weekly, has_monthly, stock_code)
                with self.lock:  # ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë³´ì¥
                    self.cursor.execute(update_query, values)
                
        except Exception as e:
            logger.error(f"âŒ ì£¼ì‹ ë§ˆìŠ¤í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ {stock_code}: {e}")

    def process_single_stock(self, stock_code):
        """ë‹¨ì¼ ì¢…ëª© ë°ì´í„° ì²˜ë¦¬ (ë²Œí¬ ì²˜ë¦¬)"""
        result = {
            'stock_code': stock_code,
            'daily_inserted': 0,
            'weekly_inserted': 0,
            'monthly_inserted': 0,
            'errors': []
        }
        
        try:
            # ì¼ë³„ ë°ì´í„° ì²˜ë¦¬
            daily_file = os.path.join(self.csv_dir, f"{stock_code}_D.csv")
            if os.path.exists(daily_file):
                df_daily = self.load_csv_data(daily_file)
                if df_daily is not None:
                    result['daily_inserted'] = self.bulk_insert_daily_data(stock_code, df_daily)
                    logger.info(f"ğŸ“Š {stock_code} ì¼ë³„ ë°ì´í„° {result['daily_inserted']}ê±´ ë²Œí¬ ì²˜ë¦¬ ì™„ë£Œ")
            
            # ì£¼ë³„ ë°ì´í„° ì²˜ë¦¬
            weekly_file = os.path.join(self.csv_dir, f"{stock_code}_W.csv")
            if os.path.exists(weekly_file):
                df_weekly = self.load_csv_data(weekly_file)
                if df_weekly is not None:
                    result['weekly_inserted'] = self.bulk_insert_weekly_data(stock_code, df_weekly)
                    logger.info(f"ğŸ“ˆ {stock_code} ì£¼ë³„ ë°ì´í„° {result['weekly_inserted']}ê±´ ë²Œí¬ ì²˜ë¦¬ ì™„ë£Œ")
            
            # ì›”ë³„ ë°ì´í„° ì²˜ë¦¬
            monthly_file = os.path.join(self.csv_dir, f"{stock_code}_M.csv")
            if os.path.exists(monthly_file):
                df_monthly = self.load_csv_data(monthly_file)
                if df_monthly is not None:
                    result['monthly_inserted'] = self.bulk_insert_monthly_data(stock_code, df_monthly)
                    logger.info(f"ğŸ“… {stock_code} ì›”ë³„ ë°ì´í„° {result['monthly_inserted']}ê±´ ë²Œí¬ ì²˜ë¦¬ ì™„ë£Œ")
            
            # ë§ˆìŠ¤í„° ì •ë³´ ì—…ë°ì´íŠ¸
            has_daily = result['daily_inserted'] > 0
            has_weekly = result['weekly_inserted'] > 0
            has_monthly = result['monthly_inserted'] > 0
            
            self.update_stock_master(stock_code, has_daily, has_weekly, has_monthly)
            
            # ì»¤ë°‹
            with self.lock:  # ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë³´ì¥
                self.connection.commit()
            
        except Exception as e:
            error_msg = f"âŒ {stock_code} ì²˜ë¦¬ ì‹¤íŒ¨: {e}"
            logger.error(error_msg)
            result['errors'].append(error_msg)
            with self.lock:  # ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë³´ì¥
                self.connection.rollback()
        
        return result

    def process_all_data_parallel(self, max_workers=None):
        """ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ ì „ì²´ ë°ì´í„° ì²˜ë¦¬"""
        start_time = time.time()
        
        logger.info("ğŸš€ ì£¼ì‹ ì‹œê³„ì—´ ë°ì´í„° ë©€í‹°ìŠ¤ë ˆë”© ì²˜ë¦¬ ì‹œì‘!")
        
        # CSV íŒŒì¼ ëª©ë¡ ì¡°íšŒ
        csv_files = self.get_csv_files()
        
        # ì¢…ëª©ì½”ë“œë³„ë¡œ ê·¸ë£¹í™”
        stock_groups = {}
        for csv_file in csv_files:
            stock_code, data_type = self.parse_stock_code_from_filename(csv_file)
            if stock_code:
                if stock_code not in stock_groups:
                    stock_groups[stock_code] = []
                stock_groups[stock_code].append(data_type)
        
        all_stocks = list(stock_groups.keys())
        logger.info(f"ğŸ“ˆ ì²˜ë¦¬í•  ì¢…ëª© ìˆ˜: {len(all_stocks)}ê°œ")
        
        # ìŠ¤ë ˆë“œ ìˆ˜ ê²°ì • (CPU ì½”ì–´ ìˆ˜ì˜ 2ë°°ê¹Œì§€)
        if max_workers is None:
            max_workers = min(16, len(all_stocks))  # ìµœëŒ€ 16ê°œ ìŠ¤ë ˆë“œ
        
        logger.info(f"âš¡ ë©€í‹°ìŠ¤ë ˆë”© ì›Œì»¤ ìˆ˜: {max_workers}ê°œ")
        
        total_results = []
        
        # ë©€í‹°ìŠ¤ë ˆë”© ì²˜ë¦¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ì¢…ëª©ë³„ë¡œ ë³‘ë ¬ ì²˜ë¦¬
            future_to_stock = {executor.submit(self.process_single_stock, stock_code): stock_code for stock_code in all_stocks}
            
            # ì§„í–‰ë¥  í‘œì‹œ
            with tqdm(total=len(all_stocks), desc="ğŸš€ ë©€í‹°ìŠ¤ë ˆë”© ì²˜ë¦¬ ì§„í–‰ë¥ ") as pbar:
                for future in as_completed(future_to_stock):
                    stock_code = future_to_stock[future]
                    try:
                        result = future.result()
                        total_results.append(result)
                        pbar.update(1)
                        
                        # ê°œë³„ ì¢…ëª© ì™„ë£Œ ë¡œê·¸
                        if result['errors']:
                            logger.warning(f"âš ï¸ {stock_code} ì²˜ë¦¬ ì™„ë£Œ (ì—ëŸ¬ ìˆìŒ)")
                        else:
                            logger.info(f"âœ… {stock_code} ì²˜ë¦¬ ì™„ë£Œ")
                            
                    except Exception as e:
                        logger.error(f"âŒ {stock_code} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                        pbar.update(1)
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        end_time = time.time()
        processing_time = end_time - start_time
        
        total_daily = sum(r['daily_inserted'] for r in total_results)
        total_weekly = sum(r['weekly_inserted'] for r in total_results)
        total_monthly = sum(r['monthly_inserted'] for r in total_results)
        total_errors = sum(len(r['errors']) for r in total_results)
        
        logger.info("=" * 60)
        logger.info("ğŸ ë©€í‹°ìŠ¤ë ˆë”© ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ!")
        logger.info(f"â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
        logger.info(f"âš¡ ìŠ¤ë ˆë“œ ì›Œì»¤: {max_workers}ê°œ")
        logger.info(f"ğŸ“Š ì²˜ë¦¬ëœ ì¢…ëª©: {len(all_stocks)}ê°œ")
        logger.info(f"ğŸ“ˆ ì¼ë³„ ë°ì´í„°: {total_daily:,}ê±´")
        logger.info(f"ğŸ“Š ì£¼ë³„ ë°ì´í„°: {total_weekly:,}ê±´")
        logger.info(f"ğŸ“… ì›”ë³„ ë°ì´í„°: {total_monthly:,}ê±´")
        logger.info(f"âŒ ì—ëŸ¬: {total_errors}ê±´")
        logger.info(f"ğŸš€ ì„±ëŠ¥ í–¥ìƒ: ì˜ˆìƒ {max_workers:.1f}ë°° ë¹ ë¦„")
        logger.info("=" * 60)
        
        return total_results

    def process_all_data(self):
        """ê¸°ì¡´ ìˆœì°¨ ì²˜ë¦¬ ë°©ì‹ (í˜¸í™˜ì„± ìœ ì§€)"""
        return self.process_all_data_parallel()

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
    db_config = {
        'host': 'localhost',
        'user': 'hanazoom_user',
        'password': 'hanazoom1234!',
        'database': 'hanazoom',
        'charset': 'utf8mb4',
        'autocommit': False
    }
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    processor = StockDataProcessor(db_config)
    if not processor.connect_db():
        return
    
    try:
        # ë©€í‹°ìŠ¤ë ˆë”© ë°ì´í„° ì²˜ë¦¬ ì‹¤í–‰
        results = processor.process_all_data_parallel()
        
        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        import json
        with open('processing_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info("ğŸ’¾ ì²˜ë¦¬ ê²°ê³¼ê°€ processing_results.json íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        logger.error(f"âŒ ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    finally:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•´ì œ
        processor.disconnect_db()

if __name__ == "__main__":
    main()
